{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pretext_task_pretrain.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "interpreter": {
      "hash": "191519e8b85a5758e955ef3b07e136bb73e82760aa2e6d7f30b5d13480ac6b6e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('ECE661': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axU-Lq73BIpz",
        "outputId": "cc43a775-c44e-495e-f7bc-02d465d5edc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ECE 661/Final')\n",
        "\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT2fCYgnBE5Z"
      },
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import random\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "from transformer import TransformerEncoder\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj4uDh4JBE5f"
      },
      "source": [
        "# USE_PRETRAINED_TOKENIZER = True\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "PAD_TOKEN = '<pad>'\n",
        "UNK_TOKEN = '<unk>'\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 256\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 2\n",
        "N_LAYERS = 3\n",
        "ATTN_HEADS = 4\n",
        "DROPOUT_RATE = 0.1\n",
        "LR = 3e-4\n",
        "N_EPOCHS = 20\n",
        "\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "transformer_name = 'bert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(transformer_name)\n",
        "vocab_size = len(tokenizer)\n",
        "pad_index = PAD_INDEX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WIZQ0j-BE5g",
        "outputId": "f3d52101-e508-401f-dcef-68437e510b55"
      },
      "source": [
        "def load_imdb(base_csv:str = './IMDBDataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the IMDB dataset\n",
        "    :param base_csv: the path of the dataset file.\n",
        "    :return: train, validation and test set.\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_csv(base_csv)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(df[\"review\"], df[\"sentiment\"], test_size=0.3, random_state=42)\n",
        "    x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "    print(f'shape of train data is {x_train.shape}')\n",
        "    print(f'shape of test data is {x_test.shape}')\n",
        "    print(f'shape of valid data is {x_valid.shape}')\n",
        "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
        "\n",
        "x_train, x_valid, x_test, y_train, y_valid, y_test = load_imdb('/content/gdrive/My Drive/ECE 661/Final/IMDBDataset.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (35000,)\n",
            "shape of test data is (7500,)\n",
            "shape of valid data is (7500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJbMnrL3BE5j"
      },
      "source": [
        "def create_bag(data, span):\n",
        "    bag = []\n",
        "    reviews = [review.split('.') for review in data]\n",
        "    for review in reviews:\n",
        "        span = span\n",
        "        if len(review) < span:\n",
        "            span = len(review)\n",
        "        else:\n",
        "            pass\n",
        "        bag += ['.'.join(review[i:i+span]) for i in range(0, len(review), span)]\n",
        "    bag_size = len(bag)\n",
        "    return bag, bag_size\n",
        "\n",
        "def create_nsp_sop_data(data, bag, bag_size, span, mode):\n",
        "    sentence_a = []\n",
        "    sentence_b = []\n",
        "    label = []\n",
        "    for review in data:\n",
        "        sentences = [sentence for sentence in review.split('.') if sentence != '']\n",
        "        span = span\n",
        "        if len(sentences) < span:\n",
        "            span = len(sentences)\n",
        "        else:\n",
        "            pass\n",
        "        sentences = ['.'.join(sentences[i:i+span]) for i in range(0, len(sentences), span)]\n",
        "        num_sentences = len(sentences)\n",
        "        if num_sentences > 1:\n",
        "            start = random.randint(0, num_sentences - 2)\n",
        "            if mode == 'nsp':\n",
        "                sentence_a.append(sentences[start])\n",
        "                if random.random() > 0.5:\n",
        "                    sentence_b.append(bag[random.randint(0, bag_size - 1)])\n",
        "                    label.append(0)\n",
        "                else:\n",
        "                    sentence_b.append(sentences[start + 1])\n",
        "                    label.append(1)\n",
        "            else:\n",
        "                if random.random() > 0.5:\n",
        "                    sentence_a.append(sentences[start])\n",
        "                    sentence_b.append(sentences[start + 1])\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    sentence_a.append(sentences[start + 1])\n",
        "                    sentence_b.append(sentences[start])\n",
        "                    label.append(0)\n",
        "    return sentence_a, sentence_b, label\n",
        "\n",
        "def create_sp_data(bag):\n",
        "    sentences = []\n",
        "    label = []\n",
        "    for review in bag:\n",
        "        review = [sentence for sentence in review.split('.') if sentence != '']\n",
        "        if len(review) == 1:\n",
        "            review = '.'.join(review)\n",
        "            sentences.append(review)\n",
        "            label.append(1)\n",
        "        else:\n",
        "            if random.random() > 0.5:\n",
        "                shuffled_review = '.'.join(random.sample(review, len(review)))\n",
        "                sentences.append(shuffled_review)\n",
        "                if [sentence for sentence in shuffled_review.split('.') if sentence != ''] == review:\n",
        "                    label.append(1)\n",
        "                else:\n",
        "                    label.append(0)\n",
        "            else:\n",
        "                review = '.'.join(review)\n",
        "                sentences.append(review)\n",
        "                label.append(1)\n",
        "    return sentences, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwcgfrbBE5k"
      },
      "source": [
        "class IMDB(Dataset):\n",
        "    def __init__(self, x, y, tokenizer, max_length=256, mode = 'mlm') -> None:\n",
        "        \"\"\"\n",
        "        :param split: can be either \"train\", \"val\" or \"test\".\n",
        "        :param tokenizer: a simple tokenizer object.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.mode = mode\n",
        "\n",
        "        SPAN = 5\n",
        "        bag, bag_size = create_bag(self.x, SPAN)\n",
        "\n",
        "        if self.mode == 'mlm':\n",
        "            pass\n",
        "        elif self.mode == 'nsp':\n",
        "            self.sentence_a, self.sentence_b, self.label = create_nsp_sop_data(self.x, bag, bag_size, SPAN, 'nsp')\n",
        "        elif self.mode == 'sop':\n",
        "            self.sentence_a, self.sentence_b, self.label = create_nsp_sop_data(self.x, bag, bag_size, SPAN, 'sop')\n",
        "        elif self.mode == 'sp':\n",
        "            self.sentences, self.label = create_sp_data(bag)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        if self.mode == 'mlm':\n",
        "            self.inputs = tokenizer(self.x.iloc[idx], return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
        "            self.inputs[\"labels\"] = self.inputs['input_ids'].detach().clone()\n",
        "            rand = torch.rand(self.inputs['input_ids'].shape)\n",
        "            mask_101 = (self.inputs['input_ids']== 101)[0]\n",
        "            mask_102 = (self.inputs['input_ids'] == 102)[0]\n",
        "            mask_ran = (rand < 0.15)[0]\n",
        "            mask_pad = (self.inputs['input_ids'] == 0)[0]\n",
        "            mask_arr = ~(mask_101 | mask_102 | mask_pad) * mask_ran\n",
        "            selection = torch.flatten(mask_arr.nonzero()).tolist()\n",
        "            self.inputs['input_ids'][0, selection] = 103\n",
        "            self.inputs[\"labels\"][0, ~mask_ran | mask_pad | mask_101 | mask_102] = -100\n",
        "\n",
        "            self.label = self.inputs[\"labels\"].flatten()\n",
        "            return {\"ids\": self.inputs['input_ids'].flatten(),\n",
        "                    \"length\": self.inputs['input_ids'].shape[1],\n",
        "                    \"label\": self.label}\n",
        "        elif self.mode == 'nsp' or self.mode == 'sop':\n",
        "            self.inputs = self.tokenizer(self.sentence_a[idx], self.sentence_b[idx], \n",
        "                                        return_tensors='pt', \n",
        "                                        max_length = self.max_length, \n",
        "                                        truncation=True, \n",
        "                                        padding = 'max_length')\n",
        "            self.inputs['labels'] = torch.unsqueeze(torch.LongTensor(self.label), 1)[idx]\n",
        "\n",
        "            return {'ids': self.inputs['input_ids'].flatten(), \n",
        "                    'length': len(self.inputs['input_ids'].flatten()),\n",
        "                    'label': self.inputs['labels']}\n",
        "        elif self.mode == 'sp':\n",
        "            inputs = tokenizer(self.x.iloc[idx],\n",
        "                               return_tensors='pt',\n",
        "                               max_length=self.max_length,\n",
        "                               padding='max_length',\n",
        "                               truncation=True)\n",
        "            sents = [sent.strip() for sent in self.x.iloc[idx].split(\".\") if sent is not '']\n",
        "            permuted_idx = np.random.permutation(len(sents))\n",
        "            permuted_sents = [sents[i] for i in permuted_idx]\n",
        "            permuted_sents = \". \".join(permuted_sents)\n",
        "            label = tokenizer(permuted_sents,\n",
        "                              return_tensors='pt',\n",
        "                              max_length=self.max_length,\n",
        "                              padding='max_length',\n",
        "                              truncation=True).input_ids\n",
        "            label = label.flatten()\n",
        "\n",
        "            return {\"ids\": inputs.input_ids.flatten(),\n",
        "                    \"length\": inputs.input_ids.shape[1],\n",
        "                    \"label\": label}\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        if self.mode == 'mlm' or self.mode == 'sp':\n",
        "            return len(self.x)\n",
        "        else:\n",
        "            return len(self.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XprH9-pBE5l"
      },
      "source": [
        "def mlm_collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_label = [torch.LongTensor(i['label']) for i in batch]\n",
        "    batch_label = nn.utils.rnn.pad_sequence(batch_label, padding_value=pad_index, batch_first=True)\n",
        "    batch = {'ids': batch_ids, 'label': batch_label}\n",
        "    return batch\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "mlm_collate = functools.partial(mlm_collate, pad_index=pad_index)\n",
        "collate = functools.partial(collate, pad_index=pad_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM_YY77tBE5l"
      },
      "source": [
        "# mlm dataloader\n",
        "mlm_train_data = IMDB(x_train, y_train, tokenizer, MAX_LENGTH, mode = 'mlm')\n",
        "mlm_valid_data = IMDB(x_valid, y_valid, tokenizer, MAX_LENGTH, mode = 'mlm')\n",
        "mlm_test_data = IMDB(x_test, y_test, tokenizer, MAX_LENGTH, mode = 'mlm')\n",
        "\n",
        "mlm_train_dataloader = torch.utils.data.DataLoader(mlm_train_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate, shuffle=True)\n",
        "mlm_valid_dataloader = torch.utils.data.DataLoader(mlm_valid_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate)\n",
        "mlm_test_dataloader = torch.utils.data.DataLoader(mlm_test_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate)\n",
        "# nsp dataloader\n",
        "nsp_train_data = IMDB(x_train, y_train, tokenizer, MAX_LENGTH, mode = 'nsp')\n",
        "nsp_valid_data = IMDB(x_valid, y_valid, tokenizer, MAX_LENGTH, mode = 'nsp')\n",
        "nsp_test_data = IMDB(x_test, y_test, tokenizer, MAX_LENGTH, mode = 'nsp')\n",
        "\n",
        "nsp_train_dataloader = torch.utils.data.DataLoader(nsp_train_data, batch_size=BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "nsp_valid_dataloader = torch.utils.data.DataLoader(nsp_valid_data, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "nsp_test_dataloader = torch.utils.data.DataLoader(nsp_test_data, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "# sop dataloader\n",
        "sop_train_data = IMDB(x_train, y_train, tokenizer, MAX_LENGTH, mode = 'sop')\n",
        "sop_valid_data = IMDB(x_valid, y_valid, tokenizer, MAX_LENGTH, mode = 'sop')\n",
        "sop_test_data = IMDB(x_test, y_test, tokenizer, MAX_LENGTH, mode = 'sop')\n",
        "\n",
        "sop_train_dataloader = torch.utils.data.DataLoader(sop_train_data, batch_size=BATCH_SIZE, collate_fn=collate, shuffle=True)\n",
        "sop_valid_dataloader = torch.utils.data.DataLoader(sop_valid_data, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "sop_test_dataloader = torch.utils.data.DataLoader(sop_test_data, batch_size=BATCH_SIZE, collate_fn=collate)\n",
        "# sp dataloader \n",
        "sp_train_data = IMDB(x_train, y_train, tokenizer, MAX_LENGTH, mode = 'sp')\n",
        "sp_valid_data = IMDB(x_valid, y_valid, tokenizer, MAX_LENGTH, mode = 'sp')\n",
        "sp_test_data = IMDB(x_test, y_test, tokenizer, MAX_LENGTH, mode = 'sp')\n",
        "\n",
        "sp_train_dataloader = torch.utils.data.DataLoader(sp_train_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate, shuffle=True)\n",
        "sp_valid_dataloader = torch.utils.data.DataLoader(sp_valid_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate)\n",
        "sp_test_dataloader = torch.utils.data.DataLoader(sp_test_data, batch_size=BATCH_SIZE, collate_fn=mlm_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QysaYfSalqkt",
        "outputId": "1d088f66-3a59-4792-bfb8-96986c2caeaf"
      },
      "source": [
        "next(iter(sop_train_dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([[  101,  1049,   102,  ...,     0,     0,     0],\n",
              "         [  101,  1996,  3772,  ...,     0,     0,     0],\n",
              "         [  101,  1996, 12703,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  1996,  3772,  ...,     0,     0,     0],\n",
              "         [  101,  1037,  2210,  ...,     0,     0,     0],\n",
              "         [  101,  2059,  2009,  ...,     0,     0,     0]]),\n",
              " 'label': tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foCH-YN1BE5m"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, transformer, output_dim, freeze):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.hidden\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.transformer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, ids):\n",
        "        \"\"\"\n",
        "        :param ids: [batch size, seq len]\n",
        "        :return: prediction of size [batch size, output dim]\n",
        "        \"\"\"\n",
        "        output = self.transformer(ids)\n",
        "        cls_hidden = output[:,0,:]\n",
        "        prediction = self.fc(torch.tanh(cls_hidden))\n",
        "        return prediction\n",
        "\n",
        "class Transformer_mlm(nn.Module):\n",
        "    def __init__(self, transformer, output_dim, freeze):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.hidden\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        if freeze:\n",
        "            for param in self.transformer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, ids):\n",
        "        \"\"\"\n",
        "        :param ids: [batch size, seq len]\n",
        "        :return: prediction of size [batch size, output dim]\n",
        "        \"\"\"\n",
        "        output = self.transformer(ids)\n",
        "        cls_hidden = output\n",
        "        prediction = self.fc(torch.tanh(cls_hidden))\n",
        "        return prediction\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def train(dataloader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids)\n",
        "        loss = criterion(prediction, label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def train_mlm(dataloader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader, desc='training...', file=sys.stdout):\n",
        "        ids = batch['ids'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "        prediction = model(ids)\n",
        "        loss = criterion(prediction.transpose(1, 2), label)\n",
        "        accuracy = get_accuracy(prediction, label)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_losses.append(loss.item())\n",
        "        epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def evaluate_mlm(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids)\n",
        "            loss = criterion(prediction.transpose(1, 2), label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size = prediction.shape[0]\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / (label.shape[0] * label.shape[1])\n",
        "    return accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyYkhW-NBE5n",
        "outputId": "16e948f4-9b3a-4365-9ea5-ea7c17c6463a"
      },
      "source": [
        "# Model\n",
        "transformer = TransformerEncoder(len(tokenizer), hidden=256, n_layers=3, attn_heads=4, dropout=0.1)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')                                                       \n",
        "# -----\n",
        "mlm_model = Transformer_mlm(transformer, vocab_size, False)\n",
        "print(f'The mlm model has {count_parameters(mlm_model):,} trainable parameters')\n",
        "mlm_optimizer = optim.Adam(mlm_model.parameters(), lr=LR)\n",
        "mlm_scheduler = transformers.get_cosine_schedule_with_warmup(mlm_optimizer,num_warmup_steps = N_EPOCHS//2,\n",
        "                                                         num_training_steps = N_EPOCHS)\n",
        "mlm_criterion= nn.CrossEntropyLoss(reduction='mean')\n",
        "mlm_model = mlm_model.to(device)\n",
        "mlm_criterion = mlm_criterion.to(device)\n",
        "# -----\n",
        "nsp_model = Transformer(transformer, OUTPUT_DIM, False)    \n",
        "print(f'The model has {count_parameters(nsp_model):,} trainable parameters')\n",
        "nsp_optimizer = optim.Adam(nsp_model.parameters(), lr=LR)\n",
        "nsp_scheduler = transformers.get_cosine_schedule_with_warmup(nsp_optimizer,num_warmup_steps = N_EPOCHS//2,\n",
        "                                                         num_training_steps = N_EPOCHS)\n",
        "nsp_criterion = nn.CrossEntropyLoss()\n",
        "nsp_model = nsp_model.to(device)\n",
        "nsp_criterion = nsp_criterion.to(device)\n",
        "# -----\n",
        "sop_model = Transformer(transformer, OUTPUT_DIM, False)    \n",
        "print(f'The model has {count_parameters(sop_model):,} trainable parameters')\n",
        "sop_optimizer = optim.Adam(sop_model.parameters(), lr=LR)\n",
        "sop_scheduler = transformers.get_cosine_schedule_with_warmup(sop_optimizer,num_warmup_steps = N_EPOCHS//2,\n",
        "                                                         num_training_steps = N_EPOCHS)\n",
        "sop_criterion = nn.CrossEntropyLoss()\n",
        "sop_model = sop_model.to(device)\n",
        "sop_criterion = sop_criterion.to(device)\n",
        "# -----\n",
        "sp_model = Transformer(transformer, OUTPUT_DIM, False)    \n",
        "print(f'The model has {count_parameters(sp_model):,} trainable parameters')\n",
        "sp_optimizer = optim.Adam(sp_model.parameters(), lr=LR)\n",
        "sp_scheduler = transformers.get_cosine_schedule_with_warmup(sp_optimizer,num_warmup_steps = N_EPOCHS//2,\n",
        "                                                         num_training_steps = N_EPOCHS)\n",
        "sp_criterion = nn.CrossEntropyLoss()\n",
        "sp_model = sp_model.to(device)\n",
        "sp_criterion = sp_criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mlm model has 18,027,834 trainable parameters\n",
            "The model has 10,184,194 trainable parameters\n",
            "The model has 10,184,194 trainable parameters\n",
            "The model has 10,184,194 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBes4j_kBE5o",
        "outputId": "8fdd7019-2feb-4878-9a58-37f9f6f71323"
      },
      "source": [
        "# Start training\n",
        "mlm_best_valid_loss = float('inf')\n",
        "mlm_train_losses = []\n",
        "mlm_train_accs = []\n",
        "mlm_valid_losses = []\n",
        "mlm_valid_accs = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    mlm_train_loss, mlm_train_acc = train_mlm(mlm_train_dataloader, mlm_model, mlm_criterion, mlm_optimizer, device)\n",
        "    mlm_valid_loss, mlm_valid_acc= evaluate_mlm(mlm_valid_dataloader, mlm_model, mlm_criterion, device)\n",
        "\n",
        "    mlm_scheduler.step()\n",
        "\n",
        "    mlm_train_losses.extend(mlm_train_loss)\n",
        "    mlm_train_accs.extend(mlm_train_acc)\n",
        "    mlm_valid_losses.extend(mlm_valid_loss)\n",
        "    mlm_valid_accs.extend(mlm_valid_acc)\n",
        "\n",
        "    mlm_epoch_train_loss = np.mean(mlm_train_loss)\n",
        "    mlm_epoch_train_acc = np.mean(mlm_train_acc)\n",
        "    mlm_epoch_valid_loss = np.mean(mlm_valid_loss)\n",
        "    mlm_epoch_valid_acc = np.mean(mlm_valid_acc)\n",
        "\n",
        "    if mlm_epoch_valid_loss < mlm_best_valid_loss:\n",
        "        mlm_best_valid_loss = mlm_epoch_valid_loss\n",
        "        torch.save(mlm_model.state_dict(), '/content/gdrive/My Drive/ECE 661/Final/mlm_pretrain.pt')\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {mlm_epoch_train_loss:.3f}, train_acc: {mlm_epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {mlm_epoch_valid_loss:.3f}' valid_acc: {mlm_epoch_valid_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training...: 100%|██████████| 2188/2188 [05:57<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:40<00:00, 11.72it/s]\n",
            "epoch: 1\n",
            "train_loss: 10.393\n",
            "valid_loss: 10.402\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.73it/s]\n",
            "epoch: 2\n",
            "train_loss: 7.053\n",
            "valid_loss: 6.719\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 3\n",
            "train_loss: 6.679\n",
            "valid_loss: 6.730\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:40<00:00, 11.70it/s]\n",
            "epoch: 4\n",
            "train_loss: 6.674\n",
            "valid_loss: 6.688\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.10it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 5\n",
            "train_loss: 6.580\n",
            "valid_loss: 6.527\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.73it/s]\n",
            "epoch: 6\n",
            "train_loss: 6.453\n",
            "valid_loss: 6.381\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:40<00:00, 11.72it/s]\n",
            "epoch: 7\n",
            "train_loss: 6.306\n",
            "valid_loss: 6.155\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:40<00:00, 11.72it/s]\n",
            "epoch: 8\n",
            "train_loss: 6.115\n",
            "valid_loss: 5.944\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.10it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.73it/s]\n",
            "epoch: 9\n",
            "train_loss: 5.928\n",
            "valid_loss: 5.774\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.76it/s]\n",
            "epoch: 10\n",
            "train_loss: 5.731\n",
            "valid_loss: 5.478\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 11\n",
            "train_loss: 5.499\n",
            "valid_loss: 5.255\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.10it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 12\n",
            "train_loss: 5.331\n",
            "valid_loss: 5.106\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.76it/s]\n",
            "epoch: 13\n",
            "train_loss: 5.203\n",
            "valid_loss: 4.981\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 14\n",
            "train_loss: 5.098\n",
            "valid_loss: 4.877\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.10it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:40<00:00, 11.70it/s]\n",
            "epoch: 15\n",
            "train_loss: 5.014\n",
            "valid_loss: 4.804\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 16\n",
            "train_loss: 4.943\n",
            "valid_loss: 4.749\n",
            "training...: 100%|██████████| 2188/2188 [05:57<00:00,  6.12it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.76it/s]\n",
            "epoch: 17\n",
            "train_loss: 4.891\n",
            "valid_loss: 4.701\n",
            "training...: 100%|██████████| 2188/2188 [05:57<00:00,  6.12it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n",
            "epoch: 18\n",
            "train_loss: 4.849\n",
            "valid_loss: 4.661\n",
            "training...: 100%|██████████| 2188/2188 [05:57<00:00,  6.12it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.75it/s]\n",
            "epoch: 19\n",
            "train_loss: 4.829\n",
            "valid_loss: 4.650\n",
            "training...: 100%|██████████| 2188/2188 [05:58<00:00,  6.11it/s]\n",
            "evaluating...: 100%|██████████| 469/469 [00:39<00:00, 11.73it/s]\n",
            "epoch: 20\n",
            "train_loss: 4.813\n",
            "valid_loss: 4.639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Houb_oBE5p",
        "outputId": "67cf4784-6525-44d3-dae5-11e26a395626"
      },
      "source": [
        "# Start training\n",
        "nsp_best_valid_loss = float('inf')\n",
        "nsp_train_losses = []\n",
        "nsp_train_accs = []\n",
        "nsp_valid_losses = []\n",
        "nsp_valid_accs = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    nsp_train_loss, nsp_train_acc = train(nsp_train_dataloader, nsp_model, nsp_criterion, nsp_optimizer, device)\n",
        "    nsp_valid_loss, nsp_valid_acc = evaluate(nsp_valid_dataloader, nsp_model, nsp_criterion, device)\n",
        "\n",
        "    nsp_scheduler.step()\n",
        "\n",
        "    nsp_train_losses.extend(nsp_train_loss)\n",
        "    nsp_train_accs.extend(nsp_train_acc)\n",
        "    nsp_valid_losses.extend(nsp_valid_loss)\n",
        "    nsp_valid_accs.extend(nsp_valid_acc)\n",
        "\n",
        "    nsp_epoch_train_loss = np.mean(nsp_train_loss)\n",
        "    nsp_epoch_train_acc = np.mean(nsp_train_acc)\n",
        "    nsp_epoch_valid_loss = np.mean(nsp_valid_loss)\n",
        "    nsp_epoch_valid_acc = np.mean(nsp_valid_acc)\n",
        "\n",
        "    if nsp_epoch_valid_loss < nsp_best_valid_loss:\n",
        "        nsp_best_valid_loss = nsp_epoch_valid_loss\n",
        "        torch.save(nsp_model.state_dict(), '/content/gdrive/My Drive/ECE 661/Final/nsp_pretrain.pt')\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {nsp_epoch_train_loss:.3f}, train_acc: {nsp_epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {nsp_epoch_valid_loss:.3f}, valid_acc: {nsp_epoch_valid_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.11it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.96it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.740, train_acc: 0.503\n",
            "valid_loss: 0.730, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.06it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.02it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.704, train_acc: 0.519\n",
            "valid_loss: 0.687, valid_acc: 0.534\n",
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.95it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.24it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.687, train_acc: 0.538\n",
            "valid_loss: 0.668, valid_acc: 0.560\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.13it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.84it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.663, train_acc: 0.560\n",
            "valid_loss: 0.649, valid_acc: 0.571\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.06it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.23it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.655, train_acc: 0.567\n",
            "valid_loss: 0.650, valid_acc: 0.571\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.10it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.96it/s]\n",
            "epoch: 6\n",
            "train_loss: 0.652, train_acc: 0.574\n",
            "valid_loss: 0.646, valid_acc: 0.574\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.14it/s]\n",
            "epoch: 7\n",
            "train_loss: 0.650, train_acc: 0.580\n",
            "valid_loss: 0.649, valid_acc: 0.558\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.03it/s]\n",
            "epoch: 8\n",
            "train_loss: 0.649, train_acc: 0.582\n",
            "valid_loss: 0.654, valid_acc: 0.572\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.08it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.70it/s]\n",
            "epoch: 9\n",
            "train_loss: 0.650, train_acc: 0.579\n",
            "valid_loss: 0.655, valid_acc: 0.560\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.11it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 58.00it/s]\n",
            "epoch: 10\n",
            "train_loss: 0.652, train_acc: 0.583\n",
            "valid_loss: 0.656, valid_acc: 0.561\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.08it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.92it/s]\n",
            "epoch: 11\n",
            "train_loss: 0.652, train_acc: 0.580\n",
            "valid_loss: 0.654, valid_acc: 0.578\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.08it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.60it/s]\n",
            "epoch: 12\n",
            "train_loss: 0.652, train_acc: 0.581\n",
            "valid_loss: 0.662, valid_acc: 0.563\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.09it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.68it/s]\n",
            "epoch: 13\n",
            "train_loss: 0.649, train_acc: 0.585\n",
            "valid_loss: 0.660, valid_acc: 0.570\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.03it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.99it/s]\n",
            "epoch: 14\n",
            "train_loss: 0.652, train_acc: 0.584\n",
            "valid_loss: 0.668, valid_acc: 0.548\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.01it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.09it/s]\n",
            "epoch: 15\n",
            "train_loss: 0.668, train_acc: 0.573\n",
            "valid_loss: 0.677, valid_acc: 0.550\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.02it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.87it/s]\n",
            "epoch: 16\n",
            "train_loss: 0.662, train_acc: 0.577\n",
            "valid_loss: 0.676, valid_acc: 0.552\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.06it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.81it/s]\n",
            "epoch: 17\n",
            "train_loss: 0.652, train_acc: 0.593\n",
            "valid_loss: 0.677, valid_acc: 0.562\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.54it/s]\n",
            "epoch: 18\n",
            "train_loss: 0.643, train_acc: 0.607\n",
            "valid_loss: 0.674, valid_acc: 0.568\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.09it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.72it/s]\n",
            "epoch: 19\n",
            "train_loss: 0.639, train_acc: 0.612\n",
            "valid_loss: 0.665, valid_acc: 0.569\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.09it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.24it/s]\n",
            "epoch: 20\n",
            "train_loss: 0.636, train_acc: 0.611\n",
            "valid_loss: 0.667, valid_acc: 0.568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66fJOoCEBE5q",
        "outputId": "d93317b5-f22a-45bf-edf8-9d832d0dd396"
      },
      "source": [
        "# Start training\n",
        "sop_best_valid_loss = float('inf')\n",
        "sop_train_losses = []\n",
        "sop_train_accs = []\n",
        "sop_valid_losses = []\n",
        "sop_valid_accs = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    sop_train_loss, sop_train_acc = train(sop_train_dataloader, sop_model, sop_criterion, sop_optimizer, device)\n",
        "    sop_valid_loss, sop_valid_acc = evaluate(sop_valid_dataloader, sop_model, sop_criterion, device)\n",
        "\n",
        "    sop_scheduler.step()\n",
        "\n",
        "    sop_train_losses.extend(sop_train_loss)\n",
        "    sop_train_accs.extend(sop_train_acc)\n",
        "    sop_valid_losses.extend(sop_valid_loss)\n",
        "    sop_valid_accs.extend(sop_valid_acc)\n",
        "\n",
        "    sop_epoch_train_loss = np.mean(sop_train_loss)\n",
        "    sop_epoch_train_acc = np.mean(sop_train_acc)\n",
        "    sop_epoch_valid_loss = np.mean(sop_valid_loss)\n",
        "    sop_epoch_valid_acc = np.mean(sop_valid_acc)\n",
        "\n",
        "    if sop_epoch_valid_loss < sop_best_valid_loss:\n",
        "        sop_best_valid_loss = sop_epoch_valid_loss\n",
        "        torch.save(sop_model.state_dict(), '/content/gdrive/My Drive/ECE 661/Final/sop_pretrain.pt')\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {sop_epoch_train_loss:.3f}, train_acc: {sop_epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {sop_epoch_valid_loss:.3f}, valid_acc: {sop_epoch_valid_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.97it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.69it/s]\n",
            "epoch: 1\n",
            "train_loss: 0.744, train_acc: 0.498\n",
            "valid_loss: 0.751, valid_acc: 0.496\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 19.98it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.50it/s]\n",
            "epoch: 2\n",
            "train_loss: 0.710, train_acc: 0.504\n",
            "valid_loss: 0.698, valid_acc: 0.500\n",
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.95it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.63it/s]\n",
            "epoch: 3\n",
            "train_loss: 0.707, train_acc: 0.499\n",
            "valid_loss: 0.694, valid_acc: 0.510\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.02it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.43it/s]\n",
            "epoch: 4\n",
            "train_loss: 0.703, train_acc: 0.501\n",
            "valid_loss: 0.694, valid_acc: 0.499\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.60it/s]\n",
            "epoch: 5\n",
            "train_loss: 0.701, train_acc: 0.504\n",
            "valid_loss: 0.694, valid_acc: 0.502\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.07it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.45it/s]\n",
            "epoch: 6\n",
            "train_loss: 0.700, train_acc: 0.498\n",
            "valid_loss: 0.698, valid_acc: 0.499\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.42it/s]\n",
            "epoch: 7\n",
            "train_loss: 0.698, train_acc: 0.503\n",
            "valid_loss: 0.695, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.00it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 56.75it/s]\n",
            "epoch: 8\n",
            "train_loss: 0.698, train_acc: 0.497\n",
            "valid_loss: 0.694, valid_acc: 0.498\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 19.99it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 56.77it/s]\n",
            "epoch: 9\n",
            "train_loss: 0.697, train_acc: 0.503\n",
            "valid_loss: 0.695, valid_acc: 0.497\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.02it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.06it/s]\n",
            "epoch: 10\n",
            "train_loss: 0.697, train_acc: 0.503\n",
            "valid_loss: 0.705, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.03it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.41it/s]\n",
            "epoch: 11\n",
            "train_loss: 0.698, train_acc: 0.497\n",
            "valid_loss: 0.696, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.00it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.54it/s]\n",
            "epoch: 12\n",
            "train_loss: 0.698, train_acc: 0.502\n",
            "valid_loss: 0.694, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.01it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.55it/s]\n",
            "epoch: 13\n",
            "train_loss: 0.698, train_acc: 0.502\n",
            "valid_loss: 0.702, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 19.98it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.48it/s]\n",
            "epoch: 14\n",
            "train_loss: 0.697, train_acc: 0.498\n",
            "valid_loss: 0.717, valid_acc: 0.497\n",
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.98it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.85it/s]\n",
            "epoch: 15\n",
            "train_loss: 0.696, train_acc: 0.500\n",
            "valid_loss: 0.694, valid_acc: 0.497\n",
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.97it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.33it/s]\n",
            "epoch: 16\n",
            "train_loss: 0.696, train_acc: 0.499\n",
            "valid_loss: 0.695, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.05it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.34it/s]\n",
            "epoch: 17\n",
            "train_loss: 0.695, train_acc: 0.503\n",
            "valid_loss: 0.700, valid_acc: 0.496\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.02it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.24it/s]\n",
            "epoch: 18\n",
            "train_loss: 0.694, train_acc: 0.504\n",
            "valid_loss: 0.694, valid_acc: 0.510\n",
            "training...: 100%|██████████| 2178/2178 [01:48<00:00, 20.01it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 57.25it/s]\n",
            "epoch: 19\n",
            "train_loss: 0.693, train_acc: 0.506\n",
            "valid_loss: 0.695, valid_acc: 0.503\n",
            "training...: 100%|██████████| 2178/2178 [01:49<00:00, 19.96it/s]\n",
            "evaluating...: 100%|██████████| 467/467 [00:08<00:00, 56.95it/s]\n",
            "epoch: 20\n",
            "train_loss: 0.693, train_acc: 0.503\n",
            "valid_loss: 0.694, valid_acc: 0.506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsayOKSHBE5r"
      },
      "source": [
        "# Start training\n",
        "sp_best_valid_loss = float('inf')\n",
        "sp_train_losses = []\n",
        "sp_train_accs = []\n",
        "sp_valid_losses = []\n",
        "sp_valid_accs = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    sp_train_loss, sp_train_acc = train_mlm(sp_train_dataloader, sp_model, sp_criterion, sp_optimizer, device)\n",
        "    sp_valid_loss, sp_valid_acc = evaluate_mlm(sp_valid_dataloader, sp_model, sp_criterion, device)\n",
        "\n",
        "    sp_scheduler.step()\n",
        "\n",
        "    sp_train_losses.extend(sp_train_loss)\n",
        "    sp_train_accs.extend(sp_train_acc)\n",
        "    sp_valid_losses.extend(sp_valid_loss)\n",
        "    sp_valid_accs.extend(sp_valid_acc)\n",
        "\n",
        "    sp_epoch_train_loss = np.mean(sp_train_loss)\n",
        "    sp_epoch_train_acc = np.mean(sp_train_acc)\n",
        "    sp_epoch_valid_loss = np.mean(sp_valid_loss)\n",
        "    sp_epoch_valid_acc = np.mean(sp_valid_acc)\n",
        "\n",
        "    if sp_epoch_valid_loss < sp_best_valid_loss:\n",
        "        sp_best_valid_loss = sp_epoch_valid_loss\n",
        "        torch.save(sp_model.state_dict(), '/content/gdrive/My Drive/ECE 661/Final/sp_pretrain.pt')\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train_loss: {sp_epoch_train_loss:.3f}, train_acc: {sp_epoch_train_acc:.3f}')\n",
        "    print(f'valid_loss: {sp_epoch_valid_loss:.3f}, valid_acc: {sp_epoch_valid_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XifjZKuRBE5s"
      },
      "source": [
        "# fig = plt.figure(figsize=(10,6))\n",
        "# ax = fig.add_subplot(1,1,1)\n",
        "# ax.plot(train_losses, label='train loss')\n",
        "# ax.plot(valid_losses, label='valid loss')\n",
        "# plt.legend()\n",
        "# ax.set_xlabel('updates')\n",
        "# ax.set_ylabel('loss')\n",
        "\n",
        "# fig = plt.figure(figsize=(10,6))\n",
        "# ax = fig.add_subplot(1,1,1)\n",
        "# ax.plot(train_accs, label='train accuracy')\n",
        "# ax.plot(valid_accs, label='valid accuracy')\n",
        "# plt.legend()\n",
        "# ax.set_xlabel('updates')\n",
        "# ax.set_ylabel('accuracy')\n",
        "\n",
        "# model.load_state_dict(torch.load('/content/gdrive/My Drive/ECE 661/Final/nsp.pt'))\n",
        "\n",
        "# test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "\n",
        "# epoch_test_loss = np.mean(test_loss)\n",
        "# epoch_test_acc = np.mean(test_acc)\n",
        "# print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}